{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitbasecondade1c09a68b1449d89643e3825835df12",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from MCTS import MCTS\n",
    "from model import PolicyNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1587119114.9983168"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "t = time.time()\n",
    "1+1\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_env = gym.make('gym_go:go-v0', size=GRID, reward_method='real')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_action = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "go_env.turn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([2., 2., 2.])"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "L = [(1, 2, 3), (1, 2, 3), (1, 2, 3)]\n",
    "np.mean(L, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_env1 = copy.copy(go_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0   1   2   3   4   5   6\n  -----------------------------\n0 | . | B | . | . | . | . | . |\n  -----------------------------\n1 | . | . | . | . | . | . | . |\n  -----------------------------\n2 | . | . | . | . | . | . | . |\n  -----------------------------\n3 | . | . | . | . | . | . | . |\n  -----------------------------\n4 | . | . | . | . | . | . | . |\n  -----------------------------\n5 | . | . | . | . | . | . | . |\n  -----------------------------\n6 | . | . | . | . | . | . | . |\n  -----------------------------\n\tTurn: W, Last Turn Passed: False, Game Over: 0\n\tBlack Area: 49.0, White Area: 0.0\n\n"
    }
   ],
   "source": [
    "state, reward, done, info = go_env.step(first_action)\n",
    "go_env.render('terminal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "go_env.game_ended()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'reg'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-22b17ba45683>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPolicyNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGRID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGRID\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mMcts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMCTS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'reg'"
     ]
    }
   ],
   "source": [
    "mod = PolicyNet(GRID, 5, GRID**2+1)\n",
    "Mcts = MCTS(mod, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\ndataset = tf.data.Dataset.from_tensors(HIST)\\ndataset = dataset.batch(batch_size, drop_remainder=True).repeat()\\niterator = list(dataset.as_numpy_iterator())\\nfor epoch in range(epochs):\\n    for batch in iterator:\\n        x, y = batch\\n        \\n        board, ind = x # Inputs: board as a numpy array and passing indicator\\n        pie, z = y # Labels: Target probability distribution and winning indicator\\n\\n        with tf.GradientTape() as tape:\\n            loss = MctsT.guidingNet.get_loss(board, ind, pie, z, training=True) # compute the loss\\n            variables = MctsT.guidingNet.trainable_variables # get the trainable variables\\n            gradients = tape.gradients(loss, variables) # compute the gradients of the loss wrt those variables\\n\\n        optimizer.apply_gradients(zip(gradients, variables)) # update the trainable weights\\n'"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "\"\"\"\n",
    "dataset = tf.data.Dataset.from_tensors(HIST)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True).repeat()\n",
    "iterator = list(dataset.as_numpy_iterator())\n",
    "for epoch in range(epochs):\n",
    "    for batch in iterator:\n",
    "        x, y = batch\n",
    "        \n",
    "        board, ind = x # Inputs: board as a numpy array and passing indicator\n",
    "        pie, z = y # Labels: Target probability distribution and winning indicator\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = MctsT.guidingNet.get_loss(board, ind, pie, z, training=True) # compute the loss\n",
    "            variables = MctsT.guidingNet.trainable_variables # get the trainable variables\n",
    "            gradients = tape.gradients(loss, variables) # compute the gradients of the loss wrt those variables\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, variables)) # update the trainable weights\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(state): \n",
    "    # Takes a state as given by env.step and return the numpy array for the board and the passing indicator\n",
    "    P0 = state[0] # 2D array of the pieces of P0\n",
    "    P1 = state[1] # 2D array of the pieces of P1\n",
    "    turn = state[2][0] # =0 if it's P0's turn =1 if P1's\n",
    "    prev_pass =  state[4][0] # Indicicator for whether the previous turn was a pass\n",
    "    if turn == 0: # Board representation, 1 for the current player's pieces -1 for the other's\n",
    "        board = P0 - P1\n",
    "    else :\n",
    "        board = P1 - P0\n",
    "    return board, prev_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=5e-3\n",
    "nb_data_gather_games=100\n",
    "max_time_step = 100\n",
    "temperature = 1\n",
    "c = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "False\n"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "attempt to get argmax of an empty sequence",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-c2a40229cc47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mmax_time_step\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;31m#and avg_v > min_val:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0mpie\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMctsD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[0mHIST\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpie\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGRID\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpie\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mb:\\Option\\Proj\\MCTS.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, initState, temperature, c, timeLimit)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtimeLimit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuteRound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'n'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mb:\\Option\\Proj\\MCTS.py\u001b[0m in \u001b[0;36mexecuteRound\u001b[1;34m(self, c)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misLeaf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#until we find a lead node\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m             \u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mb:\\Option\\Proj\\MCTS.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, node, c)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mna\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mtm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqa\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpa\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mna\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mbest_action_ix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[0mbest_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbest_action_ix\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbest_action\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'node'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mB:\\Programs\\Anaconda\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[1;34m(a, axis, out)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m     \"\"\"\n\u001b[1;32m-> 1186\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mB:\\Programs\\Anaconda\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: attempt to get argmax of an empty sequence"
     ]
    }
   ],
   "source": [
    "\n",
    "env_state = gym.make('gym_go:go-v0', size=GRID, reward_method='real')\n",
    "\n",
    "\n",
    "MctsD = MCTS(PolicyNet(GRID, 5, GRID**2+1, reg=0.1), 300)\n",
    "MctsT = MCTS(PolicyNet(GRID, 5, GRID**2+1, reg=0.1), 300)\n",
    "\n",
    "optimizer = tf.optimizers.Adam(lr)\n",
    "while True:\n",
    "    HIST = []\n",
    "    for game_ix in range(nb_data_gather_games):\n",
    "        state = env_state.reset()\n",
    "        t=0\n",
    "        while t<max_time_step : #and avg_v > min_val:\n",
    "            \n",
    "            pie = MctsD.search(env_state, temperature, c)\n",
    "            HIST.append((to_numpy(state), pie, env_state.turn()))\n",
    "            action = np.random.choice(GRID**2+1, p=pie)\n",
    "            state, _, _, _ = env_state.step(action)\n",
    "            if env_state.game_ended():\n",
    "                break\n",
    "            t += 1\n",
    "        z = 0 if env.get_winning()==1 else 1\n",
    "    HIST = [((board, ind), (pie, z==turn_id)) for (board, ind), pie, turn_id in HIST]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensors(HIST)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True).repeat()\n",
    "    iterator = list(dataset.as_numpy_iterator())\n",
    "    for epoch in range(epochs):\n",
    "        for batch in iterator:\n",
    "            x, y = batch\n",
    "            \n",
    "            board, ind = x # Inputs: board as a numpy array and passing indicator\n",
    "            pie, z = y # Labels: Target probability distribution and winning indicator\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = MctsT.guidingNet.get_loss(board, ind, pie, z, training=True) # compute the loss\n",
    "                variables = MctsT.guidingNet.trainable_variables # get the trainable variables\n",
    "                gradients = tape.gradients(loss, variables) # compute the gradients of the loss wrt those variables\n",
    "\n",
    "            optimizer.apply_gradients(zip(gradients, variables)) # update the trainable weights\n",
    "        \n",
    "        \n",
    "    t_score = 0\n",
    "    for i in range(tournament_len):\n",
    "        # Tournamnet to see would wins in a MCTS battle between the training net and the data generation net\n",
    "        env_state.reset()\n",
    "        A = ['t', 'd'] if i%2 else ['d', 't'] # to alternate who starts\n",
    "        D = {'t' : MctsT,  'd' : MctsD}\n",
    "        t=0\n",
    "        while t<max_time_step : # and avg_v > min_val:\n",
    "            for k in A:\n",
    "                pie = D[k].search(env_state, temperature, c)\n",
    "                action = np.random.choice(GRID_SIZE**2+1, p=pie)\n",
    "                env_state.step(action)\n",
    "                if env_state.game_ended():\n",
    "                    break\n",
    "                t+=1\n",
    "            if env_state.game_ended():\n",
    "                    break\n",
    "\n",
    "        z = env_state.get_winning()\n",
    "\n",
    "        if z == 1:\n",
    "            # A[0] won cuz he started\n",
    "            if A[0] == 't':\n",
    "                t_score += 1\n",
    "            else :\n",
    "                t_score -= 1\n",
    "        elif z == -1:\n",
    "            # A[1] won cuz he didnt start\n",
    "            if A[1] == 't':\n",
    "                t_score += 1\n",
    "            else :\n",
    "                t_score -= 1\n",
    "    \n",
    "    # Here t_score represent the game_won-game_lost for the training network \n",
    "    if t_score>0 : \n",
    "        # If the training net won more games that means it's better so we use it for the next MCTS data gathering\n",
    "        MctsD.guidingNet = copy.copy(MctsT.guidingNet)\n",
    "\n",
    "\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'self_play' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-d6a35774fddb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mself_play\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'self_play' is not defined"
     ]
    }
   ],
   "source": [
    "self_play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}